{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7c63b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 100/10000 - Loss: 1.200   lengthscale: 0.729   noise: 0.667  Max Loss Row: 14\n",
      "Iter 200/10000 - Loss: 1.441   lengthscale: 0.766   noise: 0.636  Max Loss Row: 19\n",
      "Iter 300/10000 - Loss: 1.517   lengthscale: 0.808   noise: 0.605  Max Loss Row: 1\n",
      "Iter 400/10000 - Loss: 1.269   lengthscale: 0.860   noise: 0.585  Max Loss Row: 10\n",
      "Iter 500/10000 - Loss: 1.957   lengthscale: 0.912   noise: 0.570  Max Loss Row: 8\n",
      "Iter 600/10000 - Loss: 1.331   lengthscale: 0.964   noise: 0.555  Max Loss Row: 16\n",
      "Iter 700/10000 - Loss: 1.291   lengthscale: 1.014   noise: 0.538  Max Loss Row: 3\n",
      "Iter 800/10000 - Loss: 1.249   lengthscale: 1.060   noise: 0.519  Max Loss Row: 13\n",
      "Iter 900/10000 - Loss: 1.171   lengthscale: 1.104   noise: 0.498  Max Loss Row: 7\n",
      "Iter 1000/10000 - Loss: 1.162   lengthscale: 1.154   noise: 0.478  Max Loss Row: 15\n",
      "Iter 1100/10000 - Loss: 1.150   lengthscale: 1.202   noise: 0.459  Max Loss Row: 15\n",
      "Iter 1200/10000 - Loss: 1.975   lengthscale: 1.245   noise: 0.438  Max Loss Row: 18\n",
      "Iter 1300/10000 - Loss: 1.328   lengthscale: 1.291   noise: 0.420  Max Loss Row: 6\n",
      "Iter 1400/10000 - Loss: 1.413   lengthscale: 1.339   noise: 0.401  Max Loss Row: 1\n",
      "Iter 1500/10000 - Loss: 1.177   lengthscale: 1.386   noise: 0.383  Max Loss Row: 10\n",
      "Iter 1600/10000 - Loss: 1.245   lengthscale: 1.432   noise: 0.365  Max Loss Row: 0\n",
      "Iter 1700/10000 - Loss: 1.943   lengthscale: 1.476   noise: 0.348  Max Loss Row: 8\n",
      "Iter 1800/10000 - Loss: 1.076   lengthscale: 1.531   noise: 0.334  Max Loss Row: 7\n",
      "Iter 1900/10000 - Loss: 0.951   lengthscale: 1.582   noise: 0.317  Max Loss Row: 5\n",
      "Iter 2000/10000 - Loss: 1.121   lengthscale: 1.616   noise: 0.299  Max Loss Row: 4\n",
      "Iter 2100/10000 - Loss: 1.011   lengthscale: 1.667   noise: 0.286  Max Loss Row: 21\n",
      "Iter 2200/10000 - Loss: 1.107   lengthscale: 1.711   noise: 0.272  Max Loss Row: 4\n",
      "Iter 2300/10000 - Loss: 1.305   lengthscale: 1.754   noise: 0.260  Max Loss Row: 11\n",
      "Iter 2400/10000 - Loss: 1.946   lengthscale: 1.805   noise: 0.251  Max Loss Row: 18\n",
      "Iter 2500/10000 - Loss: 1.017   lengthscale: 1.846   noise: 0.241  Max Loss Row: 7\n",
      "Iter 2600/10000 - Loss: 1.277   lengthscale: 1.887   noise: 0.229  Max Loss Row: 11\n",
      "Iter 2700/10000 - Loss: 1.960   lengthscale: 1.927   noise: 0.217  Max Loss Row: 18\n",
      "Iter 2800/10000 - Loss: 0.887   lengthscale: 1.965   noise: 0.208  Max Loss Row: 9\n",
      "Iter 2900/10000 - Loss: 1.072   lengthscale: 2.000   noise: 0.199  Max Loss Row: 20\n",
      "Iter 3000/10000 - Loss: 1.394   lengthscale: 2.043   noise: 0.191  Max Loss Row: 19\n",
      "Iter 3100/10000 - Loss: 1.091   lengthscale: 2.083   noise: 0.185  Max Loss Row: 10\n",
      "Iter 3200/10000 - Loss: 0.821   lengthscale: 2.121   noise: 0.181  Max Loss Row: 14\n",
      "Iter 3300/10000 - Loss: 1.339   lengthscale: 2.157   noise: 0.174  Max Loss Row: 1\n",
      "Iter 3400/10000 - Loss: 1.065   lengthscale: 2.192   noise: 0.169  Max Loss Row: 4\n",
      "Iter 3500/10000 - Loss: 1.092   lengthscale: 2.225   noise: 0.165  Max Loss Row: 10\n",
      "Iter 3600/10000 - Loss: 0.884   lengthscale: 2.267   noise: 0.159  Max Loss Row: 15\n",
      "Iter 3700/10000 - Loss: 0.911   lengthscale: 2.295   noise: 0.155  Max Loss Row: 16\n",
      "Iter 3800/10000 - Loss: 0.784   lengthscale: 2.324   noise: 0.151  Max Loss Row: 2\n",
      "Iter 3900/10000 - Loss: 0.961   lengthscale: 2.343   noise: 0.148  Max Loss Row: 7\n",
      "Iter 4000/10000 - Loss: 2.093   lengthscale: 2.369   noise: 0.146  Max Loss Row: 8\n",
      "Iter 4100/10000 - Loss: 1.032   lengthscale: 2.395   noise: 0.142  Max Loss Row: 20\n",
      "Iter 4200/10000 - Loss: 0.901   lengthscale: 2.421   noise: 0.141  Max Loss Row: 21\n",
      "Iter 4300/10000 - Loss: 1.337   lengthscale: 2.438   noise: 0.139  Max Loss Row: 1\n",
      "Iter 4400/10000 - Loss: 0.928   lengthscale: 2.464   noise: 0.136  Max Loss Row: 13\n",
      "Iter 4500/10000 - Loss: 1.247   lengthscale: 2.495   noise: 0.134  Max Loss Row: 11\n",
      "Iter 4600/10000 - Loss: 1.246   lengthscale: 2.517   noise: 0.133  Max Loss Row: 11\n",
      "Iter 4700/10000 - Loss: 1.463   lengthscale: 2.539   noise: 0.131  Max Loss Row: 19\n",
      "Iter 4800/10000 - Loss: 1.255   lengthscale: 2.562   noise: 0.131  Max Loss Row: 17\n",
      "Iter 4900/10000 - Loss: 1.334   lengthscale: 2.582   noise: 0.130  Max Loss Row: 1\n",
      "Iter 5000/10000 - Loss: 0.765   lengthscale: 2.598   noise: 0.130  Max Loss Row: 2\n",
      "Iter 5100/10000 - Loss: 0.756   lengthscale: 2.622   noise: 0.127  Max Loss Row: 14\n",
      "Loss is below the threshold. Stopping training.\n"
     ]
    }
   ],
   "source": [
    "#### This cell is for the fig1. For generating GPR samples for simulated data, please go to next cells.\n",
    "import torch\n",
    "import gpytorch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "\n",
    "subjins25_csv = pd.read_csv('SubjIns25.csv', header=None)\n",
    "subjins25_data = subjins25_csv.values\n",
    "subjins25_data = np.delete(subjins25_data, [0, 5, 10], axis=0)\n",
    "\n",
    "torch.manual_seed(231127)\n",
    "train_x = torch.tensor([22., 24., 25., 27., 30., 40., 50., 60., 70., 80., 90., 100.,\n",
    "                        120., 140., 160., 180.], dtype=torch.float32)\n",
    "subjins25 = torch.tensor(subjins25_data, dtype=torch.float32)\n",
    "train_mean = torch.mean(torch.log(subjins25), dim=0)\n",
    "train_std = torch.std(torch.log(subjins25), dim=0)\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RQKernel(\n",
    "                alpha_prior=gpytorch.priors.SmoothedBoxPrior(0.8, 1.0),\n",
    "                lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(0.01, 5.0)\n",
    "            )\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, None, likelihood)  # Set train_y to None initially\n",
    "\n",
    "model.train()\n",
    "likelihood.train()\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  \n",
    "], lr=0.001)\n",
    "\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "training_iter = 10000\n",
    "loss_threshold = 0.73\n",
    "stop_training = False\n",
    "\n",
    "for i in range(training_iter):\n",
    "    optimizer.zero_grad()\n",
    "    random_row_idx = torch.randint(0, subjins25.shape[0], (1,))\n",
    "    train_y = (torch.log(subjins25[random_row_idx]) - train_mean) / train_std\n",
    "    output = model(train_x)\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    if (i + 1) % 100 == 0:\n",
    "        max_loss_idx = random_row_idx\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f  Max Loss Row: %d' % (\n",
    "            i + 1, training_iter, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.item(),\n",
    "            model.likelihood.noise.item(),\n",
    "            max_loss_idx.item()\n",
    "        ))\n",
    "    if loss.item() < loss_threshold:\n",
    "        print('Loss is below the threshold. Stopping training.')\n",
    "        stop_training = True\n",
    "        break\n",
    "    optimizer.step()\n",
    "    if i + 1 >= training_iter:\n",
    "        print('Exceeded maximum iterations. Stopping training.')\n",
    "        stop_training = True\n",
    "        break\n",
    "\n",
    "if stop_training:\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "    num_samples = 300\n",
    "    total_samples = 5000000\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        test_x = train_x\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "\n",
    "    new_test_x = train_x\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        new_samples = observed_pred.sample(torch.Size([total_samples]))\n",
    "        pred_mean_rescaledback = torch.exp(observed_pred.mean * train_std + train_mean)\n",
    "        lower_rescaledback = torch.exp(observed_pred.confidence_region()[0] * train_std + train_mean)\n",
    "        upper_rescaledback = torch.exp(observed_pred.confidence_region()[1] * train_std + train_mean)\n",
    "\n",
    "    new_samples_rescaled = torch.exp(new_samples * train_std + train_mean)\n",
    "    new_samples_rescaled_double = new_samples_rescaled.double()\n",
    "    xdatanp = np.array(train_x.unsqueeze(0))\n",
    "\n",
    "    \n",
    "# Save necessary variables to a MATLAB-compatible mat file\n",
    "savemat('prepare_fig1_confidence_data.mat', {\n",
    "    'test_x': test_x.numpy(),\n",
    "    'lower_rescaledback': lower_rescaledback.numpy(),\n",
    "    'upper_rescaledback': upper_rescaledback.numpy(),\n",
    "    'subjins25': subjins25.numpy()\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb73e799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20869021",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Above is for fig1\n",
    "\n",
    "######## SEPARATION LINE #############\n",
    "######## SEPARATION LINE #############\n",
    "######## SEPARATION LINE #############\n",
    "######## SEPARATION LINE #############\n",
    "######## SEPARATION LINE #############\n",
    "######## SEPARATION LINE #############\n",
    "\n",
    "### Below is for the 3D and 2D models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0380fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import gpytorch\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import savemat\n",
    "\n",
    "subjins25_csv = pd.read_csv('SubjIns25alltime.csv', header=None)\n",
    "subjins25_data = subjins25_csv.values\n",
    "subjins25_data = np.delete(subjins25_data, [0, 5, 10], axis=0)\n",
    "\n",
    "subjFFA25_csv = pd.read_csv('SubjFFA25alltime.csv', header=None)\n",
    "subjglu25_csv = pd.read_csv('Subjglu25alltime.csv', header=None)\n",
    "\n",
    "\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(240328)\n",
    "\n",
    "# Define the training data\n",
    "train_x = torch.tensor([0.,2.,3.,4.,5.,6.,8.,10.,12.,14.,16.,19.,22., 24., 25., 27., 30., 40., 50., 60., 70., 80., 90., 100.,\n",
    "                        120., 140., 160., 180.], dtype=torch.float32)\n",
    "\n",
    "# train_x = torch.tensor([22., 24., 25., 27., 30., 40., 50., 60., 70., 80., 90., 100.,\n",
    "#                         120., 140., 160., 180.], dtype=torch.float32)\n",
    "subjins25 = torch.tensor(subjins25_data, dtype=torch.float32)\n",
    "\n",
    "train_mean = torch.mean(torch.log(subjins25), dim=0)\n",
    "train_std = torch.std(torch.log(subjins25), dim=0)\n",
    "\n",
    "# We will use the simplest form of GP model, exact inference\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RQKernel(\n",
    "                alpha_prior=gpytorch.priors.SmoothedBoxPrior(0.8, 1.0),\n",
    "                lengthscale_prior=gpytorch.priors.SmoothedBoxPrior(0.01, 5.0)\n",
    "            )\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Initialize likelihood and model\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(train_x, None, likelihood)  # Set train_y to None initially\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the Adam optimizer\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.parameters()},  # Includes GaussianLikelihood parameters\n",
    "], lr=0.001)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "training_iter = 100000\n",
    "loss_threshold = 0.65\n",
    "stop_training = False\n",
    "\n",
    "for i in range(training_iter):\n",
    "    # Zero gradients from previous iteration\n",
    "    optimizer.zero_grad()\n",
    "    random_row_idx = torch.randint(0, subjins25.shape[0], (1,))\n",
    "    train_y = (torch.log(subjins25[random_row_idx]) - train_mean) / train_std\n",
    "\n",
    "    # Output from model\n",
    "    output = model(train_x)\n",
    "    # Calculate loss and backpropagate gradients\n",
    "    loss = -mll(output, train_y)\n",
    "    loss.backward()\n",
    "    if (i + 1) % 100 == 0:\n",
    "        max_loss_idx = random_row_idx\n",
    "        print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f  Max Loss Row: %d' % (\n",
    "            i + 1, training_iter, loss.item(),\n",
    "            model.covar_module.base_kernel.lengthscale.item(),\n",
    "            model.likelihood.noise.item(),\n",
    "            max_loss_idx.item()\n",
    "        ))\n",
    "        # Check if the loss is below the threshold\n",
    "    if loss.item() < loss_threshold:\n",
    "        print('Loss is below the threshold. Stopping training.')\n",
    "        stop_training = True\n",
    "        break\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    # Check if the iteration exceeds the limit\n",
    "    if i + 1 >= training_iter:\n",
    "        print('Exceeded maximum iterations. Stopping training.')\n",
    "        stop_training = True\n",
    "        break\n",
    "\n",
    "if stop_training:\n",
    "    # Get into evaluation (predictive posterior) mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    num_samples = 300\n",
    "    total_samples = 5000000\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        test_x = train_x\n",
    "        observed_pred = likelihood(model(test_x))\n",
    "\n",
    "    # Generate new samples\n",
    "    new_test_x = train_x\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        new_samples = observed_pred.sample(torch.Size([total_samples]))\n",
    "        pred_mean_rescaledback = torch.exp(observed_pred.mean * train_std + train_mean)\n",
    "        lower_rescaledback = torch.exp(observed_pred.confidence_region()[0] * train_std + train_mean)\n",
    "        upper_rescaledback = torch.exp(observed_pred.confidence_region()[1] * train_std + train_mean)\n",
    "\n",
    "    new_samples_rescaled = torch.exp(new_samples * train_std + train_mean)\n",
    "\n",
    "    # Save samples to a MATLAB-compatible mat file\n",
    "#     savemat('samples.mat', {'samples': new_samples_rescaled.numpy()})\n",
    "\n",
    "    print(new_samples_rescaled.shape)\n",
    "\n",
    "\n",
    "    # Convert new_samples_rescaled to double precision\n",
    "    new_samples_rescaled_double = new_samples_rescaled.double()\n",
    "\n",
    "    # Save samples to a MATLAB-compatible mat file as double precision\n",
    "#     savemat('5millionInsulinGPRsamples_230529.mat', {'samples': new_samples_rescaled_double.numpy()})\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    xdatanp = np.array(train_x.unsqueeze(0))\n",
    "    # Plot the new samples\n",
    "    f, axs = plt.subplots(3, 1, figsize=(10, 18))\n",
    "\n",
    "    # Plot the generated samples\n",
    "    for i in range(num_samples):\n",
    "        axs[0].plot(new_test_x.numpy(), new_samples_rescaled[i].numpy(), 'b')\n",
    "\n",
    "    # Manually plot the confidence interval as a shaded region\n",
    "    confidence_region = axs[0].fill_between(test_x.numpy(), lower_rescaledback.numpy(),\n",
    "                                            upper_rescaledback.numpy(), color='red', alpha=0.5)\n",
    "\n",
    "    for i in range(subjins25.shape[0]):\n",
    "        training_data = axs[0].plot(xdatanp, subjins25[i].unsqueeze(0).numpy(), 'k*')\n",
    "\n",
    "    # Add the confidence region to the legend\n",
    "    axs[0].legend([confidence_region], ['Confidence'], loc='upper right')\n",
    "    axs[0].set_title('Plot with Default Axes Limits')\n",
    " \n",
    "    # Set axes limits for the second plot\n",
    "    axs[1].set_xlim([40, 180])\n",
    "    axs[1].set_ylim([0, 50])\n",
    "\n",
    "    # Plot the generated samples\n",
    "    for i in range(num_samples):\n",
    "        axs[1].plot(new_test_x.numpy(), new_samples_rescaled[i].numpy(), 'b')\n",
    "\n",
    "    # Manually plot the confidence interval as a shaded region\n",
    "    confidence_region = axs[1].fill_between(test_x.numpy(), lower_rescaledback.numpy(),\n",
    "                                            upper_rescaledback.numpy(), color='red', alpha=0.5)\n",
    "\n",
    "    # Plot rescaled training data as black stars\n",
    "    for i in range(subjins25.shape[0]):\n",
    "        training_data = axs[1].plot(xdatanp, subjins25[i].unsqueeze(0).numpy(), 'k*')\n",
    "\n",
    "    # Add the confidence region to the legend\n",
    "    axs[1].legend([training_data[0]], ['Training data'], loc='upper right')\n",
    "    axs[1].legend([confidence_region], ['Confidence'], loc='upper right')\n",
    "    axs[1].set_title('Plot with Custom Axes Limits ([40, 180, 0, 50])')\n",
    "\n",
    "    # Set axes limits for the third plot\n",
    "    axs[2].set_xlim([0, 40])\n",
    "    axs[2].set_ylim([0, 400])\n",
    "\n",
    "    # Plot rescaled training data as black stars\n",
    "    for i in range(subjins25.shape[0]):\n",
    "        training_data = axs[2].plot(xdatanp, subjins25[i].unsqueeze(0).numpy(), 'k*')\n",
    "\n",
    "    # Plot the generated samples\n",
    "    for i in range(num_samples):\n",
    "        axs[2].plot(new_test_x.numpy(), new_samples_rescaled[i].numpy(), 'b')\n",
    "\n",
    "    # Manually plot the confidence interval as a shaded region\n",
    "    confidence_region = axs[2].fill_between(test_x.numpy(), lower_rescaledback.numpy(),\n",
    "                                            upper_rescaledback.numpy(), color='red', alpha=0.5)\n",
    "\n",
    "    # Add the confidence region to the legend\n",
    "    axs[2].legend([confidence_region], ['Confidence'], loc='upper right')\n",
    "    axs[2].set_title('Plot with Custom Axes Limits ([20, 30, 50, 350])')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69955861",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This cell is for the 2D MODEL.\n",
    "\n",
    "# savemat('confidence_data_ins0125_alltime.mat', {\n",
    "#     'test_x': test_x.numpy(),\n",
    "#     'lower_rescaledback': lower_rescaledback.numpy(),\n",
    "#     'upper_rescaledback': upper_rescaledback.numpy(),\n",
    "#     'subjins25': subjins25.numpy()\n",
    "# })\n",
    "\n",
    "# Save samples to a MATLAB-compatible mat file as double precision\n",
    "savemat('*****SAVE YOUR INSULIN GPR SAMPLES HERE****.mat', {'samples': new_samples_rescaled_double.numpy()})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
